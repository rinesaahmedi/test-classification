# ðŸ§  What I Learned â€“ AI Internship Challenge: Text Classification

This project helped me understand **text preprocessing, feature engineering, and model behavior** in NLP tasks.

---

## Key Learnings

### 1. Text Preprocessing

- Learned to clean text: remove special characters, lowercase, remove stopwords.
- Applied **lemmatization** to reduce words to their base forms:
  - Helps models generalize better (e.g., "running" â†’ "run").
- Compared **spaCy vs NLTK** for speed and efficiency.

### 2. Feature Engineering

- Converted text to **TF-IDF vectors** to weigh important words more.
- Learned how preprocessing choices affect model performance.

### 3. Model Insights

- Trained **Logistic Regression**, **Naive Bayes**, and **SVM**.
  - **Naive Bayes**: simple, fast, works well with word counts.
  - **Logistic Regression**: better at weighting features, more flexible.
  - **SVM**: handles high-dimensional spaces effectively.
- Evaluated models with accuracy, precision, recall, F1-score, and **confusion matrices**.
- Observed that preprocessing (like lemmatization) can improve all models but affects each differently.

### 4. Project Skills

- Learned about **virtual environments** for reproducibility.
- Practiced modular coding: separating preprocessing, training, and prediction scripts.
- Reinforced AI concepts from university (e.g., matrices, feature importance) in practice.

---

### âœ… Summary

- **Preprocessing is crucial**: lemmatization and cleaning improve model performance.
- **Model choice matters**: Naive Bayes is fast, Logistic Regression is flexible, SVM works well with high-dimensional text.
- Gained hands-on experience with **NLP pipelines, feature engineering, and evaluation**.
